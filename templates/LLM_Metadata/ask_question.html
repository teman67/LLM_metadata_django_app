{% extends 'base.html' %}
{% load custom_filters %}

{% block content %}
<div class="container mt-3">
    <h2 class="text-center text-primary">Ask a Question</h2>
    
    <!-- Display error messages -->
    {% if messages %}
        <div class="alert alert-danger alert-dismissible fade show" role="alert">
            <button type="button" class="btn-close" data-bs-dismiss="alert" aria-label="Close"></button>
            {% for message in messages %}
                <p>{{ message }}</p>
            {% endfor %}
        </div>
    {% endif %}

    <form method="POST" class="mb-4 border p-4 rounded shadow-sm bg-light">
        {% csrf_token %}
        
        <!-- Question Input -->
        <div class="form-group">
            {{ form.question.label_tag }}
            {{ form.question|add_class:"form-control" }}
            <small class="form-text text-muted">Type your question here.</small>
        </div>
        
        <!-- Model Selection -->
        <div class="form-group mt-2">
            {{ form.model.label_tag }}
            {{ form.model|add_class:"form-select" }}
            <small class="form-text text-muted">Choose the LLM model for your query.</small>
        </div>

        <!-- Max Tokens Input -->
        <div class="form-group mt-2">
            {{ form.max_tokens.label_tag }}
            {{ form.max_tokens|add_class:"form-control" }}
            <small class="form-text text-muted">Limit the number of tokens in the response (e.g., 600).</small>
        </div>

        <!-- Temperature Input -->
        <div class="form-group mt-2">
            {{ form.temperature.label_tag }}
            {{ form.temperature|add_class:"form-control" }}
            <small class="form-text text-muted">Adjusts the randomness of the response. Higher values make outputs more diverse.</small>
        </div>

        <!-- Top K Input -->
        <div class="form-group mt-2">
            {{ form.top_k.label_tag }}
            {{ form.top_k|add_class:"form-control" }}
            <small class="form-text text-muted">Limit the number of top words for each predicted token (e.g., 40).</small>
        </div>

        <!-- Top P Input -->
        <div class="form-group mt-2">
            {{ form.top_p.label_tag }}
            {{ form.top_p|add_class:"form-control" }}
            <small class="form-text text-muted">Controls cumulative probability distribution for sampling. A value of 0.9 means only tokens within 90% of the probability mass are considered.</small>
        </div>

        <!-- Link for More Details -->
        <div class="mt-2 text-center">
            <p class="text-muted">For more details about the parameters, visit <a href="https://pypi.org/project/ollama-python/" target="_blank">Ollama Python</a>.</p>
        </div>
        
        <!-- Submit Button -->
        <div class="text-center">
            <button type="submit" class="btn btn-primary">Submit Question</button>
        </div>
    </form>

    <!-- Conversation History Section -->
    <h3 class="text-center text-secondary mt-4">Current Conversation</h3>
    <div class="conversation-history border p-3 rounded bg-light">
        {% if conversations %}
            {% for conversation in conversations %}
                <div class="conversation-item mb-2">
                    <p><strong>{{ conversation.role|capfirst }}:</strong> {{ conversation.content }}</p>
                    <p class="text-muted small">Model: {{ conversation.model_name }} | Tokens: {{ conversation.token_usage }} | Time: {{ conversation.elapsed_time }}s</p>
                </div>
                <hr>
            {% endfor %}
        {% else %}
            <p class="text-muted">No conversation started yet.</p>
        {% endif %}
    </div>
</div>
{% endblock %}
